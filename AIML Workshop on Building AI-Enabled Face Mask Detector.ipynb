{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI - Enabled Face Mask Detector\n",
    "\n",
    "### To read, write, and display Images & Videos\n",
    "We will set up a basic understanding of how we can begin learning image processing with OpenCV by reading, writing, and displaying images as well videos using OpenCV methods like imshow(), imread(), etc. \n",
    "\n",
    "### OpenCV\n",
    "Open-source library for the Computer Vision, Machine Learning & Image Processing.\n",
    "\n",
    "### Basic Functions\n",
    "After we have established a basic understanding, we will learn about various methods on images to resize, blur, etc.\n",
    "\n",
    "### Shapes and Texts\n",
    "We will learn about the various shapes that we can put the images into and with corresponding text values as their labels\n",
    "\n",
    "### Joining Images\n",
    "This section particularly focuses on how we can join the images together.\n",
    "\n",
    "\n",
    "### - Face Detection on Images, Videos, & in Real-time\n",
    "### - Face and Eyes Detection in Real-time\n",
    "### - Pedestrians Detection\n",
    "### - Face Mask Detection - Real Time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.1\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, Write & Display Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading & Displaying the Image\n",
    "\n",
    "img = cv2.imread('D:/Project/dog.jpg') #imread = To read the image from specified path\n",
    "\n",
    "cv2.imshow(\"Output\", img) #Displaying the Image\n",
    "\n",
    "cv2.waitKey(0) #Delay in Milliseconds for which we want to show the Image\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('D:/Project/dog2.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, Write & Display Videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading & Displaying the Image\n",
    "cap = cv2.VideoCapture(0) #0 - WebCam & 1 -External Camera\n",
    "\n",
    "#4 - byte identifier which specifies the format of a Video Stream\n",
    "fourcc = cv2.VideoWriter_fourcc('D','I','V','X')\n",
    "\n",
    "out = cv2.VideoWriter('D:/Project/myVideo2.avi',fourcc,20.0,(640,480)) #No. of frames/s, Frame Size\n",
    "\n",
    "#Videos are sequence of Images\n",
    "#Will add a while loop to capture the frame continuously\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if success == True:\n",
    "        \n",
    "        out.write(frame)\n",
    "        cv2.imshow(\"Video\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'): #Delay & to Break the Loop\n",
    "            break\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading & Displaying the Video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(3,640) #Width\n",
    "cap.set(4,480) #Height\n",
    "cap.set(10,255) #Brightness\n",
    "\n",
    "#Videos are sequence of Images\n",
    "#Will add a while loop to capture the frame continuously\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read() # img variable will capture the Video & success variable will tell us whether it was captured successfully or not\n",
    "    cv2.imshow(\"Video\", img)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): #Delay & to Break the Loop\n",
    "        break\n",
    "        \n",
    "cap.release() #Releases the resourcing after recording\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the Image into Gray Scale\n",
    "\n",
    "#Reading the Image\n",
    "img = cv2.imread('D:/Project/dog.jpg')\n",
    "imgGray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) #Converting Image into Gray Scale\n",
    "cv2.imshow('GrayScale Image', imgGray)\n",
    "cv2.imwrite('D:/Project/Graydog.png', imgGray)\n",
    "\n",
    "cv2.waitKey(0) #Delay in Milliseconds for which we want to show the Image\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLUR Function to the BLUR Image\n",
    "\n",
    "img = cv2.imread('D:/Project/dog.jpg')\n",
    "imgBlur = cv2.GaussianBlur(img, (59,59), 0)\n",
    "#(7,7) is the Kernal Size (Amount of Blur) which is always Odd. (3,3),(5,5),..etc.\n",
    "#0 is Sig function (Standard Deviation along X-Direction)\n",
    "\n",
    "cv2.imshow('Blurred Image', imgBlur)\n",
    "\n",
    "cv2.waitKey(0) #Delay in Milliseconds for which we want to show the Image\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edge Detector - Canny\n",
    "\n",
    "img = cv2.imread('D:/Project/dog.jpg')\n",
    "imgCanny = cv2.Canny(img, 150, 200) #Threshold Values\n",
    "\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "'''Type of the object which is unsigned integer of 8 bits \n",
    "which means the values can range from 0 to 255'''\n",
    "\n",
    "#dilate functions are used when Edges are not properly connected\n",
    "imgDilation = cv2.dilate(imgCanny, kernel, iterations = 1)\n",
    "\n",
    "#Erosion functon is used when we want to thin(Erode) the Image\n",
    "imgErosion = cv2.erode(imgDilation, kernel, iterations = 1)\n",
    "\n",
    "# Iterations is for the requied Thickness\n",
    "cv2.imshow('Canny Image', imgCanny)\n",
    "cv2.imshow('Dilation Image', imgDilation)\n",
    "cv2.imshow('Erosion Image', imgErosion)\n",
    "\n",
    "cv2.waitKey(0)# Delay in Milliseconds for which we want to show the Image\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424, 283, 3)\n"
     ]
    }
   ],
   "source": [
    "#Resizing the Image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('D:/Project/dog.jpg') #Reading the Image\n",
    "\n",
    "cv2.imshow(\"Output\", img) #Displaying the Image\n",
    "\n",
    "print(img.shape) #Shape of the Image (Height, Width, No. of Channels(RGB))\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resize = cv2.resize(img, (500,400)) #(Width, Height)\n",
    "\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Re-sized Image\", img_resize) \n",
    "\n",
    "cv2.waitKey(0) # Delay in Milliseconds for which we want to show the Image\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapes & Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "#0 means Black\n",
    "\n",
    "img = np.zeros((512, 512)) #Print a Black Image. It is Gray Scale Image\n",
    "cv2.imshow(\"Output\", img)\n",
    "print(img.shape)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 means Black\n",
    "\n",
    "img = np.zeros((512, 512, 3), np.uint8) #Print a Black Image with 3 Channels RGB.\n",
    "\n",
    "#img[:] = 255, 0, 0\n",
    "\n",
    "cv2.line(img, (50,30), (400,250), (231,255,9),3) \n",
    "   #Starting Point, Ending Point, Color, Thickness\n",
    "cv2.imshow(\"Output\", img)\n",
    "\n",
    "#print(img.shape)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rectangle\n",
    "\n",
    "cv2.rectangle(img, (20,20), (150,150), (255,136,2),cv2.FILLED)\n",
    "\n",
    "#cv2.FILLED is used to Fill the Rectangles\n",
    "\n",
    "cv2.imshow(\"Output\", img)\n",
    "\n",
    "#print(img.shape)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Circle\n",
    "\n",
    "cv2.circle(img, (400,200), 30, (0,255,255),cv2.FILLED)\n",
    "\n",
    "#cv2.FILLED is used to Fill the Rectangles\n",
    "\n",
    "cv2.imshow(\"Output\", img)\n",
    "\n",
    "#print(img.shape)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text\n",
    "\n",
    "cv2.putText(img, \"Text Here\", (300, 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0,150,0), 3)\n",
    "#                            Starting Point,   Font,              Scale,  Color, Thickness\n",
    "cv2.imshow(\"Output\", img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection on an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier(\"D:/Project/haarcascade_frontalface_default.xml\")\n",
    "img = cv2.imread('D:/Project/Guru.JPG')\n",
    "\n",
    "img = cv2.resize(img, (600,640))\n",
    "imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faces = faceCascade.detectMultiScale(imgGray, 1.1, 4)\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(img, (x,y),(x+w, y+h), (255,255,0),2)\n",
    "    \n",
    "cv2.imshow(\"Output\", img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection in Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture('D:/Project/myVideo.avi')\n",
    "\n",
    "#Videos are just a sequence of Images\n",
    "#So, WHILE LOOP has been added to capture the frames continuously\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(\"D:/Project/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    #Fame variable will capture the Video & Success variable will tell us whether it was captured successfully or not\n",
    "    \n",
    "    imgGray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = faceCascade.detectMultiScale(imgGray, 1.1, 4)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame, (x,y),(x+w, y+h), (0,255,0),2)\n",
    "        \n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release() #Release the resources after Recording\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Videos are just a sequence of Images\n",
    "#So, WHILE LOOP has been added to capture the frames continuously\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(\"D:/Project/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    #Fame variable will capture the Video & Success variable will tell us whether it was captured successfully or not\n",
    "    \n",
    "    imgGray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = faceCascade.detectMultiScale(imgGray, 1.1, 4)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame, (x,y),(x+w, y+h), (0,255,0),2)\n",
    "        \n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release() #Release the resources after Recording\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face & Eyes Detection in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(\"D:/Project/haarcascade_eye.xml\")\n",
    "faceCascade1 = cv2.CascadeClassifier(\"D:/Project/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    imgGray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    eyes = faceCascade.detectMultiScale(imgGray, 1.1, 4)\n",
    "    faces = faceCascade1.detectMultiScale(imgGray, 1.1, 4)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (255,17,17), 2)\n",
    "        \n",
    "    for (x, y, w, h) in eyes:\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (0,0,0), 2)\n",
    "        \n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release() #Release the resources after Recording\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pedestrians Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('D:/Project/Pedestrians.mp4')\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(\"D:/Project/haarcascade_fullbody.xml\")\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    imgGray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = faceCascade.detectMultiScale(imgGray, 1.1, 4)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,0),2)\n",
    "        \n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release() #Release the resources after Recording\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MASK DETECTION in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "detector = load_model(r'D:/Project/dummy.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the stream for Mask Detection in Real Time \n",
    "\n",
    "To detect the mask in a live stream we will follow the following steps:\n",
    "1. start the video stream \n",
    "2. Capture the frame from the stream\n",
    "3. Resize the frame\n",
    "4. Detect faces in the frame using haarcascade classifier\n",
    "5. Get the predictions using the saved model\n",
    "6. Depending on the results draw rectangle and put text on the faces accordingly\n",
    "\n",
    "The following are some of the important functions that we will use for our process.<br>\n",
    "img_to_array() - Converts the image to a numpy array<br>\n",
    "detectmultiscale() - Detects objects in the image<br>\n",
    "tf.expand_dims() - Inserts a dimension of length 1 and returns a tensor<br>\n",
    "tf.nn.softmax() - used for computing softmax activations<br>\n",
    "numpy.argmax() - returns the indices of the values that are maximum along the x-axis<br>\n",
    "    \n",
    "<strong>Alternatively, you can choose other face detection technique instead of haarcascade, since it is the most basic technique to detect faces. And sometimes the results are not very efficient. \n",
    "You can use the opencv caffe model for face detection for variation in your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "classifier = cv2.CascadeClassifier(r\"D:/Project/haarcascade_frontalface_default.xml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.27133943513035774\n",
      "0 0.27136080898344517\n",
      "0 0.27136080898344517\n",
      "0 0.27136080898344517\n",
      "0 0.27136080898344517\n",
      "0 0.27136080898344517\n"
     ]
    }
   ],
   "source": [
    "#Using the Loops to watch the stream in real time\n",
    "\n",
    "while True:\n",
    "    (success, frame) = cap.read() #Reading the frame from Stream\n",
    "    new_img = cv2.resize(frame, (frame.shape[1]//1, frame.shape[0]//1))\n",
    "    \n",
    "    #Resize the frame to speed up th process of detection\n",
    "    face = classifier.detectMultiScale(new_img)\n",
    "    \n",
    "    #Detecting faces from the frame (ROI)\n",
    "    for x,y,w,h in face:\n",
    "        try:\n",
    "            face_img = new_img[y:x+h, x:x+w]\n",
    "            #getting the coordinates for the face detected\n",
    "            resized = cv2.resize(face_img,(224,224))\n",
    "            #resizing the face detected to fit into the model in the shape(224,224)\n",
    "            img_array = tf.keras.preprocessing.image.img_to_array(resized)\n",
    "            #converting the detected image into an array\n",
    "            img_array = tf.expand_dims(img_array,0)\n",
    "            #expanding the dimensions to fit in the model\n",
    "            predictions = detector.predict(img_array)\n",
    "            #making predictions on the ROI\n",
    "            score = tf.nn.softmax(predictions[0])\n",
    "            #getting the results\n",
    "            label = numpy.argmax(score)\n",
    "        except Exception as e:\n",
    "            print('bad frame')\n",
    "            \n",
    "        if label == 0:\n",
    "            cv2.rectangle(new_img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "            cv2.putText(new_img,\"mask\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,255,0),2)\n",
    "        elif label == 1:\n",
    "            cv2.rectangle(new_img,(x,y),(x+w,y+h),(0,0,255),2)\n",
    "            cv2.putText(new_img,\"no_mask\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,0,255),2)\n",
    "        else:\n",
    "            None\n",
    "    #Displaying the window after predicting the outcome\n",
    "    cv2.imshow('face_window', new_img)\n",
    "    print(numpy.argmax(score), 100*numpy.max(score))\n",
    "    #waitkey to terminate the loop\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "#Release the Stream\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
